import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from PIL import Image
import pytesseract
import os

# ğŸ“ Paths
MODEL_PATH = "data/trained_extractor_model"
IMAGE_PATH = "imagen2.jpg"

# ğŸ§  ConfiguraciÃ³n OCR
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Ajusta si estÃ¡ en otra ruta
LANG = "spa"  # Cambia a "eng" si no tienes el espaÃ±ol

# ğŸ” OCR
print("ğŸ–¼ï¸ Leyendo imagen...")
image = Image.open(IMAGE_PATH).convert("RGB")
ocr_text = pytesseract.image_to_string(image, lang=LANG)

print("\nğŸ” Texto OCR:")
print(ocr_text)

# ğŸ“ Input para el modelo
input_text = ocr_text.strip()

print("\nğŸ“ Texto enviado al modelo:")
print("extrae_info: " + input_text)

# âš™ï¸ Cargar modelo
print("\nâš™ï¸ Cargando modelo...")
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)

# ğŸ”§ Tokenizar input
inputs = tokenizer(
    "extrae_info: " + input_text,
    return_tensors="pt",
    truncation=True,
    padding="max_length",
    max_length=512
).to(device)

# ğŸš€ Generar predicciÃ³n
print("\nğŸš€ Generando predicciÃ³n...")
output_ids = model.generate(
    **inputs,
    max_length=128,
    num_beams=4,
    early_stopping=True
)

# ğŸ§¾ Decodificar
predicted_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("\nğŸ§  PredicciÃ³n:")
print(predicted_text)
